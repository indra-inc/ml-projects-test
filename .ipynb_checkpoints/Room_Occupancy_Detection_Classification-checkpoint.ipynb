{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d241a003",
   "metadata": {},
   "source": [
    "## Note : \n",
    "- Naive Bayes algorithm is applicable only for classification problems\n",
    "- It's a Parametric approach\n",
    "- NB model is easy to build and particularly useful for very large datasets i.e dimensionality of the input data is very high\n",
    "- Continuous features should follow normal distribution\n",
    "- If cont. feature do not follow normal distribution , we should use transformation or different method to convert it into normal distribution\n",
    "- As this model assumes that there is an independence among predictors, it is suggested to remove correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71afaa4",
   "metadata": {},
   "source": [
    "## Data Set Information:\n",
    "\n",
    "- * Abstract: Experimental data used for binary classification (room occupancy) from Temperature,Humidity,Light and CO2. Ground-truth occupancy was obtained from time stamped pictures that were taken every minute i.e predicting whether a room or rooms are occupied based on environmental measures such as temperature, humidity, and related measures.\n",
    "- * This is a type of common time series classification problem called room occupancy classification.\n",
    "- * The dataset was collected by monitoring an office with a suite of environmental sensors and using a camera to determine if the room was occupied.\n",
    "- Attribute Information:\n",
    "\n",
    "- date time year-month-day hour:minute:second\n",
    "- Temperature, in Celsius\n",
    "- Relative Humidity, %ge\n",
    "- Light, in Lux\n",
    "- CO2, in ppm(parts per million)\n",
    "- Humidity Ratio, derived quantity from temperature and relative humidity measured in kgwater-vapor/kg-air\n",
    "- Occupancy, 0 or 1, 0 for not occupied, 1 for occupied status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c573c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef4f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readinag given txt file and creating dataframe\n",
    "\n",
    "df_training = pd.read_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\datatraining.txt\")\n",
    "df_test = pd.read_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\datatest.txt\")\n",
    "df_test_2 = pd.read_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\datatest2.txt\")\n",
    "  \n",
    "# Storing this dataframe in a csv file into the local file\n",
    "\n",
    "df_training.to_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\occu_training.csv\", \n",
    "                  index = None)\n",
    "\n",
    "df_test.to_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\occu_test.csv\", \n",
    "                  index = None)\n",
    "\n",
    "df_test_2.to_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\occu_test_2.csv\", \n",
    "                  index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b9fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "training = pd.read_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\occu_training.csv\")\n",
    "test = pd.read_csv(\"D:\\Data Science\\Certificates_Udemy_Coursera\\Prac_BM_PK_Book\\occupancy_data\\occu_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66baeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop date variable from both train and test as it is useless for this purpose\n",
    "training = training.drop('date', axis = 1)\n",
    "test = test.drop('date', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2ca6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8143, 6), (2665, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c32548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio',\n",
       "       'Occupancy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8662c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating training and test data for independent and dependent variable\n",
    "x_train = training.drop('Occupancy', axis = 1)\n",
    "y_train = training['Occupancy']\n",
    "\n",
    "x_test = test.drop('Occupancy', axis = 1)\n",
    "y_test = test['Occupancy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aceb7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_sc = sc.fit_transform(x_train)\n",
    "x_test_sc = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62314aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score_Naive Bayes Model : 0.9789\n",
      "Test Score_Naive Bayes Model : 0.9775\n"
     ]
    }
   ],
   "source": [
    "## Creating Naive Bayes Model\n",
    "naive_occu = GaussianNB()\n",
    "naive_occu.fit(x_train_sc, y_train)\n",
    "\n",
    "## Determining score of training and test dataset\n",
    "print('Training Score_Naive Bayes Model : %0.4f'%naive_occu.score(x_train_sc, y_train))\n",
    "print('Test Score_Naive Bayes Model : %0.4f'%naive_occu.score(x_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df86be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix_Naive Bayes :\n",
      " [[1638   55]\n",
      " [   5  967]]\n"
     ]
    }
   ],
   "source": [
    "## Predicting and determing accuracy of the model\n",
    "naive_pred = naive_occu.predict(x_test_sc)\n",
    "\n",
    "## Determining accuracy using Confusion Matrix\n",
    "naive_results = confusion_matrix(y_test,naive_pred)\n",
    "print('Confusion Matrix_Naive Bayes :\\n',naive_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52d0e61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ LOGISTIC REGRESSION MODEL ------------------\n",
      "Score_Logistic Reg : 0.9771\n",
      "Confusion Matrix_Logistic Reg :\n",
      " [[1639   54]\n",
      " [   7  965]]\n"
     ]
    }
   ],
   "source": [
    "## Creating a Logistic Regression Model\n",
    "print('------------------ LOGISTIC REGRESSION MODEL ------------------')\n",
    "\n",
    "logis_occu = LogisticRegression()\n",
    "\n",
    "logis_occu.fit(x_train_sc, y_train)\n",
    "\n",
    "logis_pred = logis_occu.predict(x_test_sc)\n",
    "logis_score = accuracy_score(y_test,logis_pred)\n",
    "print('Score_Logistic Reg : %0.4f'%logis_score)\n",
    "logis_results = confusion_matrix(y_test,logis_pred)\n",
    "print('Confusion Matrix_Logistic Reg :\\n',logis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897f6a9",
   "metadata": {},
   "source": [
    "#### Obs : \n",
    "- No big difference in train and test score in NB model\n",
    "- Accuracy of the NB model is slightly higher than Logistic Reg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d38ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab167314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
